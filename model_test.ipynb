{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Your CustomCNN class (unchanged)\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_channels: int, nodes: list, kernels: list, num_classes: int = 10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        assert len(nodes) == len(kernels), \"nodes and kernels must have the same length\"\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        \n",
    "        for out_channels, k in zip(nodes, kernels):\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=k//2)\n",
    "            layers.append(conv)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(nodes[-1], num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class FacialExpressionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.emotion_labels = ['Anger', 'Contempt', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
    "        \n",
    "        # Load all image paths and labels\n",
    "        for person_id in os.listdir(root_dir):\n",
    "            person_path = os.path.join(root_dir, person_id)\n",
    "            if not os.path.isdir(person_path):\n",
    "                continue\n",
    "            \n",
    "            for emotion_file in os.listdir(person_path):\n",
    "                if not emotion_file.endswith('.jpg'):\n",
    "                    continue\n",
    "                \n",
    "                emotion_name = emotion_file.replace('.jpg', '')\n",
    "                if emotion_name in self.emotion_labels:\n",
    "                    img_path = os.path.join(person_path, emotion_file)\n",
    "                    label = self.emotion_labels.index(emotion_name)\n",
    "                    self.samples.append((img_path, label))\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def preprocessing(\n",
    "    data_dir='archive/images',\n",
    "    image_size=64,\n",
    "    grayscale=False,  # Set to True for grayscale, False for RGB\n",
    "    batch_size=32,\n",
    "    train_split=0.8,\n",
    "    augment=True,\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the facial expression dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: path to the images folder\n",
    "    - image_size: resize images to this size (square)\n",
    "    - grayscale: if True, convert to grayscale (1 channel), else RGB (3 channels)\n",
    "    - batch_size: batch size for DataLoader\n",
    "    - train_split: proportion of data for training (rest for validation)\n",
    "    - augment: whether to apply data augmentation\n",
    "    - random_seed: for reproducibility\n",
    "    \"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Define transforms\n",
    "    transform_list = []\n",
    "    \n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels=1))\n",
    "    \n",
    "    transform_list.append(transforms.Resize((image_size, image_size)))\n",
    "    \n",
    "    if augment:\n",
    "        transform_list.extend([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        ])\n",
    "    \n",
    "    transform_list.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5] * (1 if grayscale else 3),\n",
    "                           std=[0.5] * (1 if grayscale else 3))\n",
    "    ])\n",
    "    \n",
    "    train_transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    # For validation, no augmentation\n",
    "    val_transform_list = [\n",
    "        transforms.Grayscale(num_output_channels=1) if grayscale else lambda x: x,\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5] * (1 if grayscale else 3),\n",
    "                           std=[0.5] * (1 if grayscale else 3))\n",
    "    ]\n",
    "    val_transform_list = [t for t in val_transform_list if callable(t) or isinstance(t, transforms.Compose)]\n",
    "    val_transform = transforms.Compose(val_transform_list)\n",
    "    \n",
    "    # Load full dataset\n",
    "    full_dataset = FacialExpressionDataset(data_dir, transform=train_transform)\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_size = int(train_split * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(random_seed)\n",
    "    )\n",
    "    \n",
    "    # Update validation dataset transform\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    input_channels = 1 if grayscale else 3\n",
    "    num_classes = 8  # 8 emotions\n",
    "    \n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"  Total samples: {len(full_dataset)}\")\n",
    "    print(f\"  Training samples: {train_size}\")\n",
    "    print(f\"  Validation samples: {val_size}\")\n",
    "    print(f\"  Input channels: {input_channels}\")\n",
    "    print(f\"  Number of classes: {num_classes}\")\n",
    "    print(f\"  Image size: {image_size}x{image_size}\")\n",
    "    \n",
    "    return train_loader, val_loader, input_channels, num_classes\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=20,\n",
    "    learning_rate=0.001,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    \"\"\"Train the model and return training history.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining on {device}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for images, labels in train_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 60)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Evaluate model and return predictions and labels.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "def plot_results(history, predictions, true_labels, emotion_labels):\n",
    "    \"\"\"Plot training history and confusion matrix.\"\"\"\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    # Plot training history\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    plt.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=emotion_labels, \n",
    "                yticklabels=emotion_labels,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(true_labels, predictions, \n",
    "                                target_names=emotion_labels, \n",
    "                                digits=3))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CONFIGURABLE PARAMETERS\n",
    "    IMAGE_SIZE = 64          # Size to resize images to\n",
    "    GRAYSCALE = False        # True for grayscale (1 channel), False for RGB (3 channels)\n",
    "    BATCH_SIZE = 32          # Batch size\n",
    "    NUM_EPOCHS = 20          # Number of training epochs\n",
    "    LEARNING_RATE = 0.001    # Learning rate\n",
    "    TRAIN_SPLIT = 0.8        # Train/validation split ratio\n",
    "    AUGMENT = True           # Whether to use data augmentation\n",
    "    \n",
    "    # CNN Architecture parameters\n",
    "    CNN_NODES = [32, 64, 128]      # Number of filters per conv layer\n",
    "    CNN_KERNELS = [3, 3, 5]        # Kernel sizes per conv layer\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FACIAL EXPRESSION CLASSIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Preprocessing\n",
    "    print(\"\\n[1/4] Preprocessing data...\")\n",
    "    train_loader, val_loader, input_channels, num_classes = preprocessing(\n",
    "        data_dir='archive/images',\n",
    "        image_size=IMAGE_SIZE,\n",
    "        grayscale=GRAYSCALE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        train_split=TRAIN_SPLIT,\n",
    "        augment=AUGMENT,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # Step 2: Create model\n",
    "    print(\"\\n[2/4] Creating model...\")\n",
    "    model = CustomCNN(\n",
    "        input_channels=input_channels,\n",
    "        nodes=CNN_NODES,\n",
    "        kernels=CNN_KERNELS,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"\\nModel architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Step 3: Train model\n",
    "    print(\"\\n[3/4] Training model...\")\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    # Step 4: Evaluate and visualize\n",
    "    print(\"\\n[4/4] Evaluating model...\")\n",
    "    predictions, true_labels = evaluate_model(model, val_loader)\n",
    "    \n",
    "    emotion_labels = ['Anger', 'Contempt', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
    "    \n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "    print(f\"\\nFinal Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "    \n",
    "    plot_results(history, predictions, true_labels, emotion_labels)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
